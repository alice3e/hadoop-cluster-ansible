---
# tasks file for hadoop-worker

- name: Download Hadoop
  get_url:
    url: "https://downloads.apache.org/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
    dest: "/tmp/hadoop-{{ hadoop_version }}.tar.gz"
    mode: '0755'

- name: Create hadoop installation directory
  file:
    path: "{{ hadoop_install_dir }}" # например, /home/alice3e/hadoop
    state: directory
    mode: '0755'

- name: Extract Hadoop archive
  unarchive:
    src: "/tmp/hadoop-{{ hadoop_version }}.tar.gz"
    dest: "{{ hadoop_install_dir }}"
    remote_src: yes

- name: Create symbolic link for Hadoop in /usr/local
  file:
    src: "{{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}"
    dest: "/usr/local/hadoop"
    state: link
  become: yes

- name: Configure JAVA_HOME in hadoop-env.sh
  lineinfile:
    path: "/usr/local/hadoop/etc/hadoop/hadoop-env.sh"
    regexp: '^export JAVA_HOME='
    line: 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64'
    state: present
  become: yes

- name: Set system-wide environment variables for Hadoop
  blockinfile:
    path: "/etc/profile.d/hadoop.sh"
    block: |
      export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
      export HADOOP_HOME=/usr/local/hadoop
      export PATH=$PATH:$HADOOP_HOME/bin
      export PATH=$PATH:$HADOOP_HOME/sbin
      export HADOOP_COMMON_HOME=$HADOOP_HOME
      export HADOOP_HDFS_HOME=$HADOOP_HOME
      export YARN_HOME=$HADOOP_HOME
      export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
    create: yes
    mode: "0755"
  become: yes

- name: Set environment variables for non-interactive sessions
  lineinfile:
    path: /etc/environment
    regexp: "^{{ item.key }}="
    line: "{{ item.key }}={{ item.value }}"
    state: present
  loop:
    - { key: 'JAVA_HOME', value: '/usr/lib/jvm/java-11-openjdk-amd64' }
    - { key: 'HDFS_DATANODE_USER', value: '{{ ansible_user }}' }
    - { key: 'YARN_NODEMANAGER_USER', value: '{{ ansible_user }}' }
  become: yes
  notify:
    - restart hadoop datanode
    - restart hadoop nodemanager

- name: Copy Hadoop configuration files
  template:
    src: "{{ item.src }}"
    dest: "/usr/local/hadoop/etc/hadoop/{{ item.dest }}"
  loop:
    # Worker-ноде нужны ВСЕ конфиги, чтобы клиенты (например, Spark) могли на них работать
    - { src: "core-site.xml.j2", dest: "core-site.xml" }
    - { src: "hdfs-site.xml.j2", dest: "hdfs-site.xml" }
    - { src: "mapred-site.xml.j2", dest: "mapred-site.xml" }
    - { src: "yarn-site.xml.j2", dest: "yarn-site.xml" }
  become: yes
  notify:
    - restart hadoop datanode
    - restart hadoop nodemanager

- name: Set correct ownership for all Hadoop files
  file:
    path: "/usr/local/hadoop"
    state: directory
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
    recurse: yes
  become: yes

- name: Set correct ownership for Hadoop files
  file:
    path: "/opt/hadoop"
    state: directory
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
    recurse: yes
  become: yes

- name: Wait for NameNode to be available
  wait_for:
    host: "{{ hostvars[groups['hadoop_master'][0]]['internal_ip'] }}"
    port: 9000
    delay: 5
    timeout: 120
    state: started

- name: Start HDFS DataNode
  shell: |
    source /etc/profile.d/hadoop.sh
    /usr/local/hadoop/bin/hdfs --daemon start datanode
  args:
    executable: /bin/bash
  become: yes
  become_user: "{{ ansible_user }}"

- name: Start YARN NodeManager
  shell: |
    source /etc/profile.d/hadoop.sh
    /usr/local/hadoop/bin/yarn --daemon start nodemanager
  args:
    executable: /bin/bash
  become: yes
  become_user: "{{ ansible_user }}"
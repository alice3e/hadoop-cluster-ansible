---
# tasks file for hadoop-worker
- name: Update linux apt
  ansible.builtin.apt:
    name: "*"
    state: latest
    update_cache: yes
  become: yes

- name: Ensure .ssh directory exists
  file:
    path: "/home/{{ ansible_user }}/.ssh"
    state: directory
    mode: '0700'

- name: Install master's public key for passwordless SSH
  authorized_key:
    user: "{{ ansible_user }}"
    key: "{{ lookup('file', 'roles/common/files/ssh/hadoop-key.pub') }}"
    state: present

- name: Install required packages (OpenJDK 11)
  apt:
    name:
      - openjdk-11-jdk
    state: present
  become: yes

- name: Download Hadoop
  get_url:
    url: "https://downloads.apache.org/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
    dest: "/tmp/hadoop-{{ hadoop_version }}.tar.gz"
    mode: '0755'

- name: Create hadoop installation directory
  file:
    path: "{{ hadoop_install_dir }}" # например, /home/alice3e/hadoop
    state: directory
    mode: '0755'

- name: Extract Hadoop archive
  unarchive:
    src: "/tmp/hadoop-{{ hadoop_version }}.tar.gz"
    dest: "{{ hadoop_install_dir }}"
    remote_src: yes

- name: Create symbolic link for Hadoop in /usr/local
  file:
    src: "{{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}"
    dest: "/usr/local/hadoop"
    state: link
  become: yes

- name: Set system-wide environment variables for Hadoop
  blockinfile:
    path: "/etc/profile.d/hadoop.sh"
    block: |
      export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
      export HADOOP_HOME=/usr/local/hadoop
      export PATH=$PATH:$HADOOP_HOME/bin
      export PATH=$PATH:$HADOOP_HOME/sbin
      # Переменные MASTER-демонов здесь не нужны
      export HADOOP_COMMON_HOME=$HADOOP_HOME
      export HADOOP_HDFS_HOME=$HADOOP_HOME
      export YARN_HOME=$HADOOP_HOME
    create: yes
    mode: "0755"
  become: yes

- name: Set environment variables for non-interactive sessions
  lineinfile:
    path: /etc/environment
    regexp: "^{{ item.key }}="
    line: "{{ item.key }}={{ item.value }}"
    state: present
  loop:
    - { key: 'JAVA_HOME', value: '/usr/lib/jvm/java-11-openjdk-amd64' }
    - { key: 'HDFS_DATANODE_USER', value: '{{ ansible_user }}' }
    - { key: 'YARN_NODEMANAGER_USER', value: '{{ ansible_user }}' }
  become: yes
  notify:
    - restart hadoop datanode
    - restart hadoop nodemanager

- name: Copy Hadoop configuration files
  template:
    src: "{{ item.src }}"
    dest: "/usr/local/hadoop/etc/hadoop/{{ item.dest }}"
  loop:
    # Worker-ноде нужны ВСЕ конфиги, чтобы клиенты (например, Spark) могли на них работать
    - { src: "core-site.xml.j2", dest: "core-site.xml" }
    - { src: "hdfs-site.xml.j2", dest: "hdfs-site.xml" }
    - { src: "mapred-site.xml.j2", dest: "mapred-site.xml" }
    - { src: "yarn-site.xml.j2", dest: "yarn-site.xml" }
  become: yes
  notify:
    - restart hadoop datanode
    - restart hadoop nodemanager

- name: Set correct ownership for all Hadoop files
  file:
    path: "/usr/local/hadoop"
    state: directory
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
    recurse: yes
  become: yes